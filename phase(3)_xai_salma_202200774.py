# -*- coding: utf-8 -*-
"""Phase(3)_XAI_Salma_202200774.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z869vbigGz23Or-YWftiRV57YA3pB1VZ

# 1) Import Libraries
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objs as go

from datetime import datetime

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import shap

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import joblib
import warnings
warnings.filterwarnings('ignore')
sns.set_style('whitegrid')

"""## Load all the Dataset"""

df_full_grouped = pd.read_csv('/content/full_grouped.csv')
df_covid_19_clean = pd.read_csv('/content/covid_19_clean_complete.csv')
df_country_wise_latest = pd.read_csv('/content/country_wise_latest.csv')
df_day_wise = pd.read_csv('/content/day_wise.csv')
df_usa_county_wise = pd.read_csv('/content/usa_county_wise.csv')
df_worldometer_data = pd.read_csv('/content/worldometer_data.csv')

# Create a list of dataframes and file names for easy processing
data_files = {
    "full_grouped": df_full_grouped,
    "covid_19_clean_complete": df_covid_19_clean,
    "country_wise_latest": df_country_wise_latest,
    "day_wise": df_day_wise,
    "usa_county_wise": df_usa_county_wise,
    "worldometer_data": df_worldometer_data
}

# display the first rows of each dataset to understand structure
for name, df in data_files.items():
    print(f"\n--- {name} ---")
    print("Shape of the data:", df.shape)
    print("Columns in the dataset:", df.columns.tolist())
    display(df.head())
    print("\n" + "-"*50)

"""## Data Cleaning"""

# fun to check  missing values & provide initial insights
def check_missing_values(df, name):
    print(f"\nChecking Missing Values for {name} dataset:")
    missing_data = df.isnull().sum()
    missing_data = missing_data[missing_data > 0]

    if not missing_data.empty:
        print(missing_data)
    else:
        print("No Missing Valyes Found.")
    print("\n" + "-"*50)

# applying the func to each dataset
for name, df in data_files.items():
    check_missing_values(df, name)

"""### Handling Missing Values and Data  Inconsistencies"""

# replace missing values in Province/State column by Unknown
df_covid_19_clean['Province/State'] = df_covid_19_clean['Province/State'].fillna('Unknown')

# - For 'FIPS', fill with -1, as it's an ID field, which could indicate absence of an official code.
df_usa_county_wise['FIPS'] = df_usa_county_wise['FIPS'].fillna(-1)

# - For 'Admin2', fill with 'Unknown' since this likely represents a county or subdivision name.
df_usa_county_wise['Admin2'] = df_usa_county_wise['Admin2'].fillna('Unknown')

# Handle Missing Values differently for categorical and numerical columns:
df_worldometer_data['Continent'] = df_worldometer_data['Continent'].fillna('Unknown')
df_worldometer_data['Population'] = df_worldometer_data['Population'].fillna(0)

# fill missing numerical values with 0 for case and testing-related fields.
df_worldometer_data.fillna({
    'NewCases': 0,
    'TotalDeaths': 0,
    'NewDeaths': 0,
    'TotalRecovered': 0,
    'NewRecovered': 0,
    'ActiveCases': 0,
    'Serious,Critical': 0,
    'Tot Cases/1M pop': 0,
    'Deaths/1M pop': 0,
    'TotalTests': 0,
    'Tests/1M pop': 0
}, inplace=True)

# fill missing values WHO Region values with "Unknown"
df_worldometer_data['WHO Region'] = df_worldometer_data['WHO Region'].fillna('Unknown')

# re-check for any remaining missing values in each dataset
print("\nPost-Cleaning Missing Value Check:")

for name, df in data_files.items():
    check_missing_values(df, name)

"""## Exploratory Data Analysis (EDA)

### 1. Basic Data Summary
"""

# fun to perform basic data summary
def basic_data_summary(df, name):
    print(f"--- Summary Statistics of {name} ---")

    # display the shape of the dataset
    print(f"Shape of the dataset:", {df.shape})

    # display datatypes of the dataset
    print("Data Types and Non-null counts:")
    print(df.info())

    # display unique values count for each col
    print("\nUnique Values Count:")
    print(df.nunique())

    # display the summary statistics for each dataset
    print("\nSummary Statistics:")
    display(df.describe(include='all'))

    print("\n" + "-"*50 + "\n")

# loop through datasets and perform data summary for each
for name, data in data_files.items():
    basic_data_summary(data, name)

"""### 2. Date Range Analysis

What is the date range covered in each dataset?
"""

# Function to get date range for each dataset
def date_range_summary(df, dataset_name, date_column):
    print(f"--- Date Range for {dataset_name} ---")

    # Convert date_column to datetime if it's not already
    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')

    # Calculate date range
    start_date = df[date_column].min()
    end_date = df[date_column].max()

    print(f"Date Range: {start_date.date()} to {end_date.date()}")
    print("\n" + "-"*50 + "\n")

# Loop through datasets and check date ranges
date_columns = {
    "full_grouped": "Date",
    "covid_19_clean_complete": "Date",
    "country_wise_latest": None,  # No date column in this dataset
    "day_wise": "Date",
    "usa_county_wise": "Date",
    "worldometer_data": None  # No date column in this dataset
}

for name, data in data_files.items():
    date_column = date_columns[name]
    if date_column:  # Only perform date range summary if a date column exists
        date_range_summary(data, name, date_column)
    else:
        print(f"--- {name} does not have a date column. ---")
        print("\n" + "-"*50 + "\n")

"""### 3. Trend Analysis

How have confirmed cases, deaths, and recoveries trended over time globally and for specific countries?
"""

# Function to plot trends
def plot_trend_analysis(df, country=None):
    # Set the style for seaborn
    sns.set(style="whitegrid")

    # Convert 'Date' to datetime if not already
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

    # Aggregate data if a specific country is not provided
    if country:
        df = df[df['Country/Region'] == country]
    else:
        df = df.groupby('Date').agg({
            'Confirmed': 'sum',
            'Deaths': 'sum',
            'Recovered': 'sum'
        }).reset_index()

    # Calculate moving averages
    df['Confirmed_MA'] = df['Confirmed'].rolling(window=7).mean()
    df['Deaths_MA'] = df['Deaths'].rolling(window=7).mean()
    df['Recovered_MA'] = df['Recovered'].rolling(window=7).mean()

    # Plotting
    plt.figure(figsize=(14, 7))

    # Confirmed cases trend
    plt.subplot(3, 1, 1)
    sns.lineplot(data=df, x='Date', y='Confirmed_MA', color='blue', label='Confirmed Cases (7-Day MA)')
    plt.title(f'Trend of Confirmed Cases over Time' if not country else f'Trend of Confirmed Cases in {country}')
    plt.xlabel('Date')
    plt.ylabel('Number of Cases')
    plt.legend()

    # Deaths trend
    plt.subplot(3, 1, 2)
    sns.lineplot(data=df, x='Date', y='Deaths_MA', color='red', label='Deaths (7-Day MA)')
    plt.title(f'Trend of Deaths over Time' if not country else f'Trend of Deaths in {country}')
    plt.xlabel('Date')
    plt.ylabel('Number of Deaths')
    plt.legend()

    # Recoveries trend
    plt.subplot(3, 1, 3)
    sns.lineplot(data=df, x='Date', y='Recovered_MA', color='green', label='Recovered (7-Day MA)')
    plt.title(f'Trend of Recoveries over Time' if not country else f'Trend of Recoveries in {country}')
    plt.xlabel('Date')
    plt.ylabel('Number of Recoveries')
    plt.legend()

    plt.tight_layout()
    plt.show()

# global trend analysis
plot_trend_analysis(df_full_grouped)

# Trend analysis for specific countries (example)
countries = ['Pakistan', 'India', 'Brazil']
for country in countries:
    plot_trend_analysis(df_full_grouped, country)

"""**Insights**:
- Plotted global and country-specific trends using 7-day moving averages for smoother visualization.
- Used subplots to display separate trends for confirmed cases, deaths, and recoveries over time.

### 4. Growth Rate
What is the daily and weekly growth rate of cases and deaths?
"""

# calulate dialy and weekly growth rates for conformed cases and deaths in the "df_full_grouped" dataset
df_full_grouped['Daily Growth Rate (Cases)'] = df_full_grouped['Confirmed'].pct_change().fillna(0) * 100
df_full_grouped['Daily Growth Rate (Deaths)'] = df_full_grouped['Deaths'].pct_change().fillna(0) * 100

# calculate weekly growth rate using a 7-day difference
df_full_grouped['Weekly Growth Rate (Cases)'] = df_full_grouped['Confirmed'].pct_change(periods=7).fillna(0) * 100
df_full_grouped['Weekly Growth Rate (Deaths)'] = df_full_grouped['Deaths'].pct_change(periods=7).fillna(0) * 100

# display the top rows with new growth rate columns for review
# Display the top rows with new growth rate columns for review
display(df_full_grouped[['Date', 'Country/Region', 'Confirmed', 'Deaths',
              'Daily Growth Rate (Cases)', 'Daily Growth Rate (Deaths)',
              'Weekly Growth Rate (Cases)', 'Weekly Growth Rate (Deaths)']].head(50))

"""**Insights:**

- Calculated daily and weekly growth rates to capture day-to-day and week-to-week spread rates.
- Growth rates show the percentage increase, aiding in tracking virus transmission speed over different intervals.

### 5. Country/Region Comparison

Which countries have the highest confirmed cases, deaths, and recoveries?
"""

# Select top 10 countries with the highest confirmed cases, deaths, and recoveries from the 'country_wise_latest' dataset
top_countries_cases = df_country_wise_latest.nlargest(10, 'Confirmed')[['Country/Region', 'Confirmed']]
top_countries_deaths = df_country_wise_latest.nlargest(10, 'Deaths')[['Country/Region', 'Deaths']]
top_countries_recovered = df_country_wise_latest.nlargest(10, 'Recovered')[['Country/Region', 'Recovered']]

# Plot bar charts for top 10 countries in confirmed cases, deaths, and recoveries
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
sns.barplot(x='Confirmed', y='Country/Region', data=top_countries_cases, ax=axes[0], palette='Blues')
axes[0].set_title('Top 10 Countries by Confirmed Cases')
sns.barplot(x='Deaths', y='Country/Region', data=top_countries_deaths, ax=axes[1], palette='Reds')
axes[1].set_title('Top 10 Countries by Deaths')
sns.barplot(x='Recovered', y='Country/Region', data=top_countries_recovered, ax=axes[2], palette='Greens')
axes[2].set_title('Top 10 Countries by Recoveries')

plt.tight_layout()
plt.show()

"""### 6. Mortality and Recovery Rates

What are the death and recovery rates by country/region?
"""

# calulate mortality and recovery rates for each country in 'df_country_wise_latest' dataset
df_country_wise_latest["Mortality Rate (%)"] = (df_country_wise_latest['Deaths'] / df_country_wise_latest['Confirmed']) * 100
df_country_wise_latest["Recovery Rate (%)"] = (df_country_wise_latest['Recovered'] / df_country_wise_latest['Confirmed']) * 100

# display the top rows with new rate columns
df_country_wise_latest[['Country/Region', 'Confirmed', 'Deaths', 'Recovered',
                     'Mortality Rate (%)', 'Recovery Rate (%)']].head(10)

"""### 7. Cases per Million

How does the infection rate per million people compare across countries?
"""

# calulate cases per million for each country in 'worldmeter_data' dataset
df_worldometer_data['Cases per Million'] = (df_worldometer_data['TotalCases'] / df_worldometer_data['Population']) * 1000000

# select top 10 countries by cases per million for visualization
df_top_countries_per_million = df_worldometer_data.nlargest(10, 'Cases per Million')[['Country/Region', 'Cases per Million']]

# plot cases per million across top 10 countries
plt.figure(figsize=(10,6))
sns.barplot(x='Cases per Million', y='Country/Region', data=df_top_countries_per_million, palette='Purples')
plt.title('Top 10 Countries by COVID-19 Cases Per Million')
plt.xlabel('Cases Per Million')
plt.ylabel('Country/Region')
plt.show()

"""### 8. Monthly and Weekly Patterns

Are there any significant monthly or weekly patterns in confirmed cases, deaths, and recoveries?
"""

# convert 'Date' column to datetime format in 'df_full_grouped' dataset
df_full_grouped['Date'] = pd.to_datetime(df_full_grouped['Date'])

# extract month and week from the data for grouping
df_full_grouped['Month'] = df_full_grouped['Date'].dt.month
df_full_grouped['Week'] = df_full_grouped['Date'].dt.isocalendar().week

# group by month and calculate the mean confirmed cases, deaths, and recoveries
monthly_data = df_full_grouped.groupby('Month')[['Confirmed', 'Deaths', 'Recovered']].mean()

# group by week and calculate the mean confirmed cases, deaths, amd recoveries
weekly_data = df_full_grouped.groupby('Week')[['Confirmed', 'Deaths', 'Recovered']].mean()

# plot monthly patterns
plt.figure(figsize=(14,6))
monthly_data.plot(kind='line', marker='o')
plt.title('Monthly Patterns in COVID-19 Confirmed Cases, Deaths, and Recoveries')
plt.xlabel('Month')
plt.ylabel('Average Counts')
plt.show()

# Plot weekly patterns
plt.figure(figsize=(14, 6))
weekly_data.plot(kind='line', marker='o')
plt.title('Weekly Patterns in COVID-19 Confirmed Cases, Deaths, and Recoveries')
plt.xlabel('Week')
plt.ylabel('Average Counts')
plt.show()

"""### 9. Top 10 Regions with High Cases Over Time

How do the top 10 affected regions' cases change over time?
"""

# identify the top 10 countries with the highest confirmed cases in 'df_country_wise_latest' dataset
top_10_countries = df_country_wise_latest.nlargest(10, 'Confirmed')['Country/Region'].tolist()

# filter the 'df_full_grouped' dataset for inly these top 10 countries
top_10_countries = df_full_grouped[df_full_grouped['Country/Region'].isin(top_10_countries)]

# plot confirmed cases over time for each of the top 10 countries
plt.figure(figsize=(14,8))
sns.lineplot(x='Date', y='Confirmed', hue='Country/Region', data=top_10_countries, palette='tab10')
plt.title('Confirmed Cases Over Time for Top 10 Affected Countries')
plt.xlabel('Date')
plt.ylabel('Confirmed Cases')
plt.legend(title='Country/Region', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.show()

"""### 10. New Cases, Deaths, and Recoveries

What are the daily new cases, deaths, and recoveries by country?
"""

# calculate daily new cases, deaths, and recoveries by country in 'full_grouped' dataset
df_full_grouped = df_full_grouped.sort_values(['Country/Region', 'Date'])

df_full_grouped['Daily New Cases'] = df_full_grouped.groupby('Country/Region')['Confirmed'].diff().fillna(0)
df_full_grouped['Daily New Deaths'] = df_full_grouped.groupby('Country/Region')['Deaths'].diff().fillna(0)
df_full_grouped['Daily New Recoveries'] = df_full_grouped.groupby('Country/Region')['Recovered'].diff().fillna(0)

# filter for a sample country, let's choose Pakistan
sample_country = df_full_grouped[df_full_grouped['Country/Region'] == 'Pakistan']

# Plot daily new cases, deaths, and recoveries for the sample country
plt.figure(figsize=(14, 8))
sns.lineplot(x='Date', y='Daily New Cases', data=sample_country, label='Daily New Cases')
sns.lineplot(x='Date', y='Daily New Deaths', data=sample_country, label='Daily New Deaths', color='red')
sns.lineplot(x='Date', y='Daily New Recoveries', data=sample_country, label='Daily New Recoveries', color='green')
plt.title('Daily New Cases, Deaths, and Recoveries in United States')
plt.xlabel('Date')
plt.ylabel('Daily Counts')
plt.legend()
plt.xticks(rotation=45)
plt.show()

"""### 11. Case Fatality Ratio (CFR) by Region

How does the case fatality ratio vary across regions?
"""

# calculate case fatality Ratio (CFR) by region in 'country_wise_latest' dataset
df_country_wise_latest['CFR (%)'] = (df_country_wise_latest['Deaths'] / df_country_wise_latest['Confirmed']) * 100

# drop any rows with NaN or infinite CFR values for better visualization
df_country_wise_latest = df_country_wise_latest.replace([np.inf, -np.inf], np.nan).dropna(subset=['CFR (%)'])

# plot top 10 countries with higest CFR
top_10_cfr = df_country_wise_latest.nlargest(10, 'CFR (%)')
plt.figure(figsize=(14,6))
sns.barplot(x='CFR (%)', y='Country/Region', data=top_10_cfr, palette='Reds')
plt.title('Top 10 Countries by Case Fatality Ratio (CFR)')
plt.xlabel('CFR (%)')
plt.ylabel('Country/Region')
plt.show()

"""### 12. Active vs. Recovered Cases

What is the relationship between active and recovered cases over time?
"""

# filter data for a sample country let's say 'India'
sample_country = df_full_grouped[df_full_grouped['Country/Region'] == 'India']

# plot active vs. recovered cases over time
plt.figure(figsize=(14, 8))
sns.lineplot(x='Date', y='Active', data=sample_country, label='Active Cases', color='blue')
sns.lineplot(x='Date', y='Recovered', data=sample_country, label='Recovered Cases', color='green')
plt.title('Active vs. Recovered Cases in India')
plt.xlabel('Date')
plt.ylabel('Counts')
plt.legend()
plt.xticks(rotation=45)
plt.show()

"""### 13. Geographical Mapping

Where are the global hotspots for COVID-19?
"""

# use 'df_worldmeter_data' for latest hotspot data with geogrpahical coordinates
fig = px.choropleth(df_worldometer_data,
                    locations="Country/Region",
                    locationmode="country names",
                    color="TotalCases",
                    hover_name="Country/Region",
                    hover_data=["TotalDeaths", "TotalRecovered", "ActiveCases"],
                    color_continuous_scale="Reds",
                    title="Global COVID-19 Hotspots (Total Cases)")

fig.update_geos(showcoastlines=True, coastlinecolor="Black")
fig.update_layout(coloraxis_colorbar=dict(title="Total Cases"))
fig.show()

"""### 14. Impact by Population Density

How does population density affect infection rates across countries?
"""

df_worldometer_data.columns

# calculate cases per million using 'df_worldometer_data' dataset
df_worldometer_data['Cases per Million'] = df_worldometer_data['TotalCases'] / df_worldometer_data['Population'] * 1e6

# plot population density vs. cases per million
plt.figure(figsize=(14, 8))
sns.scatterplot(x='Population', y='Cases per Million', data=df_worldometer_data, hue='Continent', alpha=0.7)
plt.title('Population Density vs. Cases per Million')
plt.xlabel('Population Density')
plt.ylabel('Cases per Million')
plt.legend()
plt.show()

"""### 15. Continent Analysis

What are the differences in case distribution, deaths, and recoveries by continent?
"""

# 1. Total Cases, Deaths, and Recoveries by Continent
# Group by continent and calculate the sum of cases, deaths, and recoveries
continent_data = df_worldometer_data.groupby('Continent')[['TotalCases', 'TotalDeaths', 'TotalRecovered']].sum()

# Normalize by population for comparison
continent_data['Cases per Million'] = (continent_data['TotalCases'] / df_worldometer_data.groupby('Continent')['Population'].sum()) * 1e6
continent_data['Deaths per Million'] = (continent_data['TotalDeaths'] / df_worldometer_data.groupby('Continent')['Population'].sum()) * 1e6
continent_data['Recoveries per Million'] = (continent_data['TotalRecovered'] / df_worldometer_data.groupby('Continent')['Population'].sum()) * 1e6

# Plotting the continent-level summary
continent_data[['Cases per Million', 'Deaths per Million', 'Recoveries per Million']].plot(kind='bar', stacked=True, figsize=(14, 8))
plt.title('COVID-19 Cases, Deaths, and Recoveries per Million by Continent')
plt.xlabel('Continent')
plt.ylabel('Per Million')
plt.show()

# 2. Recovery Rate and Death Rate by Continent
# Calculate Death Rate and Recovery Rate by continent
continent_data['Death Rate'] = (continent_data['TotalDeaths'] / continent_data['TotalCases']) * 100
continent_data['Recovery Rate'] = (continent_data['TotalRecovered'] / continent_data['TotalCases']) * 100

# Plotting Death and Recovery Rate by continent
continent_data[['Death Rate', 'Recovery Rate']].plot(kind='bar', figsize=(12, 8), color=['red', 'green'])
plt.title('Death Rate and Recovery Rate by Continent')
plt.xlabel('Continent')
plt.ylabel('Percentage')
plt.show()

"""### 16. WHO Region Comparison

How do different WHO regions compare in terms of cases, deaths, and recoveries?
"""

# grouping data by WHO region and summing the total cases
region_cases = df_worldometer_data.groupby('WHO Region')['TotalCases'].sum().sort_values(ascending=False)

# plotting the bar chart for total cases by WHO region
plt.figure(figsize=(12, 8))
sns.barplot(x=region_cases.index, y=region_cases.values, palette='Blues_d')
plt.title('Total COVID-10 Cases by WHO Region')
plt.xlabel('WHO Region')
plt.ylabel('Total Cases')
plt.xticks(rotation=45)
plt.show()

## 2. bar chart for deaths and recoveries by who region
region_deaths_recoveries = df_worldometer_data.groupby('WHO Region')[['TotalDeaths', 'TotalRecovered']].sum()

# plotting the bar chart for deaths and recoveries by WHO region
region_deaths_recoveries.plot(kind='bar', figsize=(12, 8), color=['red', 'green'])
plt.title('Deaths and Recoveries by WHO Region')
plt.xlabel('WHO Region')
plt.ylabel('Counts')
plt.xticks(rotation=45)
plt.legend(['Deaths', 'Recoveries'])
plt.show()

"""### 17. Total Tests vs. Confirmed Cases
Is there a correlation between testing and the number of confirmed cases?
"""

# Plotting a scatter plot to visualize the relationship between Total Tests and Confirmed Cases
plt.figure(figsize=(10, 6))
sns.scatterplot(x='TotalTests', y='TotalCases', data=df_worldometer_data, color='blue', alpha=0.7)
plt.title('Total Tests vs. Confirmed Cases')
plt.xlabel('Total Tests')
plt.ylabel('Total Confirmed Cases')
plt.show()

from scipy.stats import pearsonr

# Calculating the Pearson correlation coefficient between Total Tests and Total Cases
correlation, _ = pearsonr(df_worldometer_data['TotalTests'], df_worldometer_data['TotalCases'])

print(f"Pearson Correlation Coefficient between Total Tests and Confirmed Cases: {correlation:.2f}")

"""### 18. Distribution by Pie Charts"""

# Plotting the distribution of Total Cases by Continent using a pie chart
continent_cases = df_worldometer_data.groupby('Continent')['TotalCases'].sum()
plt.figure(figsize=(8, 8))
continent_cases.plot(kind='pie', autopct='%1.1f%%', startangle=90, cmap='Set3')
plt.title('Distribution of Total Cases by Continent')
plt.ylabel('')
plt.show()

# Plotting the distribution of Active Cases by Continent using a pie chart
continent_active_cases = df_worldometer_data.groupby('Continent')['ActiveCases'].sum()
plt.figure(figsize=(8, 8))
continent_active_cases.plot(kind='pie', autopct='%1.1f%%', startangle=90, cmap='YlGnBu')
plt.title('Distribution of Active Cases by Continent')
plt.ylabel('')
plt.show()

# Plotting the proportion of Serious Critical Cases
serious_critical = df_worldometer_data['Serious,Critical'].sum()
non_serious = df_worldometer_data['ActiveCases'].sum() - serious_critical

# Create a pie chart for Serious vs Non-Serious Critical Cases
labels = ['Serious Critical', 'Non-Serious Critical']
sizes = [serious_critical, non_serious]
colors = ['#FF6666', '#66B3FF']
plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
plt.title('Proportion of Serious Critical Cases')
plt.show()

# Plotting the distribution of Recovered, Active, and Death cases using a pie chart
recovered_cases = df_worldometer_data['TotalRecovered'].sum()
active_cases = df_worldometer_data['ActiveCases'].sum()
death_cases = df_worldometer_data['TotalDeaths'].sum()

# Create a pie chart for Recovered, Active, and Death cases
labels = ['Recovered', 'Active', 'Deaths']
sizes = [recovered_cases, active_cases, death_cases]
colors = ['#28a745', '#17a2b8', '#dc3545']
plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
plt.title('Recovered, Active, and Death Cases Worldwide')
plt.show()

"""## Time Series Analysis

### 1. Creating a Hypothetical Date Column
"""

# Create a hypothetical date column for time series analysis
df_worldometer_data['Date'] = pd.date_range(start="2020-01-01", periods=len(df_worldometer_data), freq='D')

# Verify the new 'Date' column
df_worldometer_data.head()

"""### 2. Time Series Analysis using the Hypothetical Date Column"""

# Grouping data by Date to aggregate total cases, deaths, and recoveries
df_time_series = df_worldometer_data.groupby(['Date'])[['TotalCases', 'TotalDeaths', 'TotalRecovered']].sum().reset_index()

# Checking the first few rows of the aggregated data
df_time_series.head()

# Plotting the total cases, deaths, and recoveries over time
plt.figure(figsize=(14, 8))

# Plotting Total Cases, Total Deaths, and Total Recovered over time
plt.plot(df_time_series['Date'], df_time_series['TotalCases'], label='Total Cases', color='blue')
plt.plot(df_time_series['Date'], df_time_series['TotalDeaths'], label='Total Deaths', color='red', linestyle='--')
plt.plot(df_time_series['Date'], df_time_series['TotalRecovered'], label='Total Recovered', color='green', linestyle=':')

# Add titles and labels
plt.title('COVID-19 Total Cases, Deaths, and Recoveries Over Time')
plt.xlabel('Date')
plt.ylabel('Total Count')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Calculate 7-day moving averages
df_time_series['MovingAvg_Cases'] = df_time_series['TotalCases'].rolling(window=7).mean()
df_time_series['MovingAvg_Deaths'] = df_time_series['TotalDeaths'].rolling(window=7).mean()
df_time_series['MovingAvg_Recovered'] = df_time_series['TotalRecovered'].rolling(window=7).mean()

# Plotting the moving averages
plt.figure(figsize=(14, 8))

# Plotting moving averages
plt.plot(df_time_series['Date'], df_time_series['MovingAvg_Cases'], label='7-day Moving Avg - Cases', linewidth=2)
plt.plot(df_time_series['Date'], df_time_series['MovingAvg_Deaths'], label='7-day Moving Avg - Deaths', linewidth=2, linestyle='--')
plt.plot(df_time_series['Date'], df_time_series['MovingAvg_Recovered'], label='7-day Moving Avg - Recovered', linewidth=2, linestyle=':')

# Add titles and labels
plt.title('COVID-19 7-day Moving Averages for Cases, Deaths, and Recoveries')
plt.xlabel('Date')
plt.ylabel('Moving Average Count')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""### 3. Detecting Significant Events Using Z-Score"""

from scipy.stats import zscore

# Calculate Z-scores for Total Cases, Total Deaths, and Total Recovered
df_time_series['Zscore_Cases'] = zscore(df_time_series['TotalCases'])
df_time_series['Zscore_Deaths'] = zscore(df_time_series['TotalDeaths'])
df_time_series['Zscore_Recovered'] = zscore(df_time_series['TotalRecovered'])

# Check the Z-scores
df_time_series[['Date', 'Zscore_Cases', 'Zscore_Deaths', 'Zscore_Recovered']].head()

# Define a threshold for significant events
threshold = 2

# Mark significant events where the Z-score is beyond the threshold
df_time_series['Significant_Event_Cases'] = df_time_series['Zscore_Cases'].apply(lambda x: x > threshold or x < -threshold)
df_time_series['Significant_Event_Deaths'] = df_time_series['Zscore_Deaths'].apply(lambda x: x > threshold or x < -threshold)
df_time_series['Significant_Event_Recovered'] = df_time_series['Zscore_Recovered'].apply(lambda x: x > threshold or x < -threshold)

# Check the rows with significant events
significant_events = df_time_series[df_time_series[['Significant_Event_Cases', 'Significant_Event_Deaths', 'Significant_Event_Recovered']].any(axis=1)]
significant_events[['Date', 'TotalCases', 'Zscore_Cases', 'Significant_Event_Cases', 'TotalDeaths', 'Zscore_Deaths', 'Significant_Event_Deaths', 'TotalRecovered', 'Zscore_Recovered', 'Significant_Event_Recovered']]

plt.figure(figsize=(14, 8))

# Plotting Total Cases, Total Deaths, and Total Recovered over time
plt.plot(df_time_series['Date'], df_time_series['TotalCases'], label='Total Cases', color='blue')
plt.plot(df_time_series['Date'], df_time_series['TotalDeaths'], label='Total Deaths', color='red', linestyle='--')
plt.plot(df_time_series['Date'], df_time_series['TotalRecovered'], label='Total Recovered', color='green', linestyle=':')

# Highlighting significant events
plt.scatter(df_time_series['Date'][df_time_series['Significant_Event_Cases']],
            df_time_series['TotalCases'][df_time_series['Significant_Event_Cases']],
            color='blue', label='Significant Events (Cases)', zorder=5, marker='o', s=20)
plt.scatter(df_time_series['Date'][df_time_series['Significant_Event_Deaths']],
            df_time_series['TotalDeaths'][df_time_series['Significant_Event_Deaths']],
            color='red', label='Significant Events (Deaths)', zorder=5, marker='o', s=20)

plt.scatter(df_time_series['Date'][df_time_series['Significant_Event_Recovered']],
            df_time_series['TotalRecovered'][df_time_series['Significant_Event_Recovered']],
            color='green', label='Significant Events (Recovered)', zorder=5, marker='o', s=20)

# Add titles and labels
plt.title('Significant Events in COVID-19 Cases, Deaths, and Recoveries Over Time')
plt.xlabel('Date')
plt.ylabel('Total Count')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""### 4. Using IQR (Interquartile Range)"""

# Calculate IQR for Total Cases, Total Deaths, and Total Recovered
Q1_cases = df_time_series['TotalCases'].quantile(0.25)
Q3_cases = df_time_series['TotalCases'].quantile(0.75)
IQR_cases = Q3_cases - Q1_cases

lower_bound_cases = Q1_cases - 1.5 * IQR_cases
upper_bound_cases = Q3_cases + 1.5 * IQR_cases

# Detect outliers for Total Cases
df_time_series['Significant_Event_Cases_IQR'] = (df_time_series['TotalCases'] < lower_bound_cases) | (df_time_series['TotalCases'] > upper_bound_cases)

# Repeat for Total Deaths and Total Recovered
Q1_deaths = df_time_series['TotalDeaths'].quantile(0.25)
Q3_deaths = df_time_series['TotalDeaths'].quantile(0.75)
IQR_deaths = Q3_deaths - Q1_deaths

lower_bound_deaths = Q1_deaths - 1.5 * IQR_deaths
upper_bound_deaths = Q3_deaths + 1.5 * IQR_deaths

df_time_series['Significant_Event_Deaths_IQR'] = (df_time_series['TotalDeaths'] < lower_bound_deaths) | (df_time_series['TotalDeaths'] > upper_bound_deaths)

# Repeat for Total Recovered
Q1_recovered = df_time_series['TotalRecovered'].quantile(0.25)
Q3_recovered = df_time_series['TotalRecovered'].quantile(0.75)
IQR_recovered = Q3_recovered - Q1_recovered

lower_bound_recovered = Q1_recovered - 1.5 * IQR_recovered
upper_bound_recovered = Q3_recovered + 1.5 * IQR_recovered

df_time_series['Significant_Event_Recovered_IQR'] = (df_time_series['TotalRecovered'] < lower_bound_recovered) | (df_time_series['TotalRecovered'] > upper_bound_recovered)

plt.figure(figsize=(14, 8))

# Plotting Total Cases, Total Deaths, and Total Recovered over time
plt.plot(df_time_series['Date'], df_time_series['TotalCases'], label='Total Cases', color='blue')
plt.plot(df_time_series['Date'], df_time_series['TotalDeaths'], label='Total Deaths', color='red', linestyle='--')
plt.plot(df_time_series['Date'], df_time_series['TotalRecovered'], label='Total Recovered', color='green', linestyle=':')

# Highlighting significant events using IQR
plt.scatter(df_time_series['Date'][df_time_series['Significant_Event_Cases_IQR']],
            df_time_series['TotalCases'][df_time_series['Significant_Event_Cases_IQR']],
            color='blue', label='Significant Events (Cases)', zorder=5, marker='o', s=20)

plt.scatter(df_time_series['Date'][df_time_series['Significant_Event_Deaths_IQR']],
            df_time_series['TotalDeaths'][df_time_series['Significant_Event_Deaths_IQR']],
            color='red', label='Significant Events (Deaths)', zorder=5, marker='o', s=20)

plt.scatter(df_time_series['Date'][df_time_series['Significant_Event_Recovered_IQR']],
            df_time_series['TotalRecovered'][df_time_series['Significant_Event_Recovered_IQR']],
            color='green', label='Significant Events (Recovered)', zorder=5, marker='o', s=20)

# Add titles and labels
plt.title('Significant Events in COVID-19 Cases, Deaths, and Recoveries Over Time (IQR Method)')
plt.xlabel('Date')
plt.ylabel('Total Count')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

"""# 5) ML Modeling : 2nd Paper
[Over- and under-estimation of COVID-19 deaths](https://link.springer.com/article/10.1007/s10654-021-00787-9)

Criteria | Meaning | Class
Low Deaths / 100 Cases < 1.0 | Suggests over-reporting or effective healthcare | 1
High confirmed cases (>100k) + low New deaths / New cases | Suggests over-attribution or high testing | 1
High Deaths / 100 Cases > 5.0 | Suggests undercounting or poor healthcare access | 0
Low confirmed cases (<10k) | Suggests lack of detection/testing | 0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import shap
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Load dataset
df = pd.read_csv('country_wise_latest.csv')

# Keep relevant columns
cols = [
    'Country/Region', 'Confirmed', 'Deaths', 'Recovered', 'Active',
    'New cases', 'New deaths',
    'Deaths / 100 Cases', 'Recovered / 100 Cases', 'Deaths / 100 Recovered'
]
df = df[cols].copy()

# Handle missing/infinite values
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(0, inplace=True)

# Add label: 0 = Undercounted, 1 = Overcounted (Adjusted Logic)
df['Label'] = 0  # default

df.loc[
    (df['Deaths / 100 Cases'] < 2.0) &
    (df['Confirmed'] > 50000) &
    ((df['New deaths'] / (df['New cases'] + 1)) < 0.05),
    'Label'
] = 1

# Drop rows with 0 confirmed
df = df[df['Confirmed'] > 0].copy()

# Balance the dataset (undersample class 0 to match class 1)
df_0 = df[df['Label'] == 0].sample(n=30, random_state=42)
df_1 = df[df['Label'] == 1]
df_balanced = pd.concat([df_0, df_1])

# Check label distribution
print("Balanced label distribution:")
print(df_balanced['Label'].value_counts())

# Extract features and target for next steps
X = df_balanced.drop(columns=['Country/Region', 'Label'])
y = df_balanced['Label']

# Split features and target
X = df.drop(columns=['Country/Region', 'Label'])
y = df['Label']

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# Train Random Forest
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluation
print("\n Classification Report:")
print(classification_report(y_test, y_pred))
print(" Accuracy:", accuracy_score(y_test, y_pred))

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Under-counted', 'Over-counted'],
            yticklabels=['Under-counted', 'Over-counted'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""# SHAP VALUES."""

import shap

explainer = shap.TreeExplainer(model)

shap_values = explainer.shap_values(X_test)

# Convert X_test to DataFrame (if not already)
X_test_df = pd.DataFrame(X_test, columns=feature_names)

# Summary bar plot (mean absolute SHAP value)
shap.summary_plot(shap_values, X_test_df, plot_type="bar")

"""#SVM"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Train SVM model
svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train, y_train)

# Predict
y_pred_svm = svm_model.predict(X_test)

# Evaluation
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("\n Classification Report (SVM):")
print(classification_report(y_test, y_pred_svm))

print("\n Confusion Matrix (SVM):")
print(confusion_matrix(y_test, y_pred_svm))

"""#SHAP with SVM"""

import shap

X_sample = shap.utils.sample(X_test, 100, random_state=42)

X_sample_df = pd.DataFrame(X_sample, columns=feature_names)

background = shap.utils.sample(X_train, 100, random_state=42)

explainer = shap.KernelExplainer(svm_model.predict_proba, background)

shap_values = explainer.shap_values(X_sample_df)


shap.summary_plot(shap_values, X_sample_df, plot_type='bar')







"""#  Paper 3
https://www.ncbi.nlm.nih.gov/books/NBK554776/

This notebook analyzes simulated patient data reflecting the clinical features and treatment outcomes of COVID-19, inspired by the NCBI article "Features, Evaluation, and Treatment of Coronavirus (COVID-19)" .
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""# Data collection

"""

np.random.seed(42)
n_samples = 1000
data = pd.DataFrame({
    'Age': np.random.randint(20, 80, size=n_samples),
    'Fever': np.random.choice([0, 1], size=n_samples),
    'Cough': np.random.choice([0, 1], size=n_samples),
    'Fatigue': np.random.choice([0, 1], size=n_samples),
    'Shortness_of_Breath': np.random.choice([0, 1], size=n_samples),
    'Comorbidities': np.random.choice([0, 1], size=n_samples),
    'Treatment': np.random.choice(['Supportive', 'Antiviral', 'Monoclonal Antibodies'], size=n_samples),
    'Outcome': np.random.choice([0, 1], size=n_samples)
})

"""# Data Preprocessing"""

# Encode categorical variables
le = LabelEncoder()
data['Treatment'] = le.fit_transform(data['Treatment'])

# Features and target
X = data.drop('Outcome', axis=1)
y = data['Outcome']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Model Training"""

svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train_scaled, y_train)

"""# Model Evaluation"""

# Predict
y_pred = svm_model.predict(X_test_scaled)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""# Model Interpretability with SHAP"""

import shap

X_sample = shap.utils.sample(X_test_scaled, 100, random_state=42)

background = shap.utils.sample(X_train_scaled, 100, random_state=42)

explainer = shap.KernelExplainer(svm_model.predict_proba, background)

shap_values = explainer.shap_values(X_sample)

X_sample_df = pd.DataFrame(X_sample, columns=X.columns)


shap.summary_plot(shap_values, X_sample_df, plot_type='bar')



import pandas as pd
import numpy as np
import shap
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('country_wise_latest.csv')
df.head()

"""# Preprocess Data"""

df['HighRisk'] = (df['Deaths / 100 Cases'] > 3.0).astype(int)

df_model = df.drop(['Country/Region', 'WHO Region'], axis=1, errors='ignore')

features_to_drop = ['Deaths', 'New deaths', 'Deaths / 100 Recovered']
df_model = df_model.drop(columns=features_to_drop, errors='ignore')

df_model.fillna(0, inplace=True)

X = df_model.drop('HighRisk', axis=1)
y = df_model['HighRisk']

"""# Split and Scale"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Train SVM"""

from sklearn.svm import SVC

svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train_scaled, y_train)

"""# Evaluate"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

y_pred = svm_model.predict(X_test_scaled)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""# SHAP"""

import shap

# Sample for SHAP efficiency
X_sample = shap.utils.sample(X_test_scaled, 50, random_state=42)
background = shap.utils.sample(X_train_scaled, 50, random_state=42)

# KernelExplainer for SVM
explainer = shap.KernelExplainer(svm_model.predict_proba, background)
shap_values = explainer.shap_values(X_sample)

# Summary plot
X_sample_df = pd.DataFrame(X_sample, columns=X.columns)
shap.summary_plot(shap_values, X_sample_df, plot_type="bar")

"""# ML Modeling :XGBoost
[COVID-19 prediction models: a systematic literature review](https://ophrp.org/journal/view.php?number=626)
"""

import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

df = pd.read_csv('full_grouped.csv')
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(0, inplace=True)

# Create target variable
df['Severity'] = (df['New deaths'] > df['New deaths'].median()).astype(int)

# Define leakage-free features
features = ['New cases', 'New recovered']
X = df[features]
y = df['Severity']
groups = df['Country/Region']  # used for country-wise splitting

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Group-based split (no same country in both train/test)
gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in gss.split(X_scaled, y, groups):
    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

# Train XGBoost
model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

import shap

X_test_df = pd.DataFrame(X_test, columns=features)

# Create SHAP explainer and compute SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Plot SHAP summary plot
shap.summary_plot(shap_values, X_test_df, feature_names=features, show=True)

"""# Ml Modeling : Logistic regression."""

from sklearn.linear_model import LogisticRegression

# Load and clean the data
df = pd.read_csv('full_grouped.csv')
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(0, inplace=True)

# Label: high severity if New deaths > median
df['Severity'] = (df['New deaths'] > df['New deaths'].median()).astype(int)

# Features (no leakage)
features = ['New cases', 'New recovered']
X = df[features]
y = df['Severity']
groups = df['Country/Region']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Group-wise train/test split
gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in gss.split(X_scaled, y, groups):
    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

# Train logistic regression
model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("Classification Report:")
print(classification_report(y_test, y_pred))
print(" Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

X_test_df = pd.DataFrame(X_test, columns=features)

explainer = shap.KernelExplainer(model.predict_proba, X_train[:100])
shap_values = explainer.shap_values(X_test_df)

class_names = ['0', '1']

shap.summary_plot(shap_values, X_test_df, feature_names=features, class_names=class_names, show=True, class_inds=[1]) # Specifying the index of the class



"""#LIME"""

!pip install lime

import lime
import lime.lime_tabular

lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train,
    feature_names=features,
    class_names=['Low Severity', 'High Severity'],
    mode='classification'
)

i = 0
lime_exp = lime_explainer.explain_instance(
    data_row=X_test[i],
    predict_fn=model.predict_proba,
    num_features=2
)

lime_exp.show_in_notebook()

"""#Partial Dependence Plot (PDP)"""

from sklearn.inspection import PartialDependenceDisplay

pdp_features = [0, 1]
PartialDependenceDisplay.from_estimator(
    model, X_test, pdp_features,
    feature_names=features,
    target=1  # Class: High severity
)

"""#Permutation Importance"""

from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import numpy as np

perm_result = permutation_importance(
    model, X_test, y_test,
    n_repeats=10, random_state=42, n_jobs=-1
)

sorted_idx = perm_result.importances_mean.argsort()
plt.figure(figsize=(8, 5))
plt.barh(np.array(features)[sorted_idx], perm_result.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance")
plt.title("Logistic Regression: Permutation Importance")
plt.tight_layout()
plt.show()





!pip install scikit-learn -U
!pip install lime

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
import shap
import lime
import lime.lime_tabular
from sklearn.inspection import permutation_importance
from sklearn.inspection import partial_dependence
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings("ignore")


# Load dataset
df = pd.read_csv('country_wise_latest.csv')

# Keep relevant columns
cols = [
    'Country/Region', 'Confirmed', 'Deaths', 'Recovered', 'Active',
    'New cases', 'New deaths',
    'Deaths / 100 Cases', 'Recovered / 100 Cases', 'Deaths / 100 Recovered'
]
df = df[cols].copy()

# Handle missing/infinite values
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(0, inplace=True)

# Add label: 0 = Undercounted, 1 = Overcounted (Adjusted Logic)
df['Label'] = 0  # default

df.loc[
    (df['Deaths / 100 Cases'] < 2.0) &
    (df['Confirmed'] > 50000) &
    ((df['New deaths'] / (df['New cases'] + 1)) < 0.05),
    'Label'
] = 1

# Drop rows with 0 confirmed
df = df[df['Confirmed'] > 0].copy()

# Balance the dataset (undersample class 0 to match class 1)
df_0 = df[df['Label'] == 0].sample(n=30, random_state=42)
df_1 = df[df['Label'] == 1]
df_balanced = pd.concat([df_0, df_1])

# Check label distribution
print("Balanced label distribution:")
print(df_balanced['Label'].value_counts())

# Extract features and target for next steps
X = df_balanced.drop(columns=['Country/Region', 'Label'])
y = df_balanced['Label']

# Split features and target
X = df.drop(columns=['Country/Region', 'Label'])
y = df['Label']

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y)

# Train Random Forest
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    "SVM": SVC(probability=True),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": XGBClassifier(eval_metric='logloss'),
    "Decision Tree": DecisionTreeClassifier(),
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB()
}

results = {}

for name, model in models.items():
    print(f"\n--- {name} ---")
    model.fit(X_train_scaled, y_train)
    preds = model.predict(X_test_scaled)
    acc = accuracy_score(y_test, preds)
    print(f"Accuracy: {acc:.4f}")

# SHAP
if name == "SVM":
    explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_train_scaled, 100))
elif name in ["Random Forest", "Decision Tree", "XGBoost"]:
    explainer = shap.TreeExplainer(model)
else:
   explainer = shap.KernelExplainer(model.predict_proba, shap.sample(X_train_scaled, 100))

shap_values = explainer.shap_values(X_test_scaled[:100])
shap.summary_plot(shap_values, X_test_scaled[:100], feature_names=X.columns.tolist())
plt.title(f"SHAP Summary for {name}")
plt.show()

# LIME
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
X_train_scaled, feature_names=X.columns.tolist(), class_names=["No", "Yes"], discretize_continuous=True
)
lime_exp = lime_explainer.explain_instance(X_test_scaled[0], model.predict_proba, num_features=5)
lime_exp.show_in_notebook(show_table=True)

# Permutation Importance
perm = permutation_importance(model, X_test_scaled, y_test, n_repeats=10, random_state=42)
sorted_idx = perm.importances_mean.argsort()[::-1][:5]
plt.barh(np.array(X.columns)[sorted_idx], perm.importances_mean[sorted_idx])
plt.title(f"Permutation Importance: {name}")
plt.gca().invert_yaxis()
plt.show()

# Partial Dependence
pdp_results = partial_dependence(
        model, X_train_scaled, features=[0, 1], feature_names=X.columns.tolist()
)
fig, ax = plt.subplots()
pdp_values = pdp_results["average"]
pdp_features = pdp_results.grid_values[0]

num_target_classes = pdp_values.shape[0]
for i, target in enumerate(np.unique(y_train)):
    if i < num_target_classes:
       ax.plot(pdp_features, pdp_values[i, :], label=f"Target {target}")
else:
    print(f"Warning: No PDP data for target class {target}")

ax.set_xlabel(X.columns[0])
ax.set_ylabel("Average Partial Dependence")
ax.set_title(f"PDP for {name}")
ax.legend()
plt.tight_layout()
plt.show()

results[name] = acc

# Summary
print("\nModel Accuracies:")
for model, acc in results.items():
    print(f"{model}: {acc:.4f}")

